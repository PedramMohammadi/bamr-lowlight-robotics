# Lowâ€‘Light Enhancement for Robotics Applications

Brightness-Aware Mini Restorer (BAMR) is a lightweight image enhancer trained to improve roboticsâ€‘relevant downstream metrics (feature matching & object detection) in low light conditions. Unlike classic lowâ€‘light methods that optimize PSNR/SSIM only, **BAMR is taskâ€‘aware**. after a paired pretraining stage, it is tuned to boost LoFTR matching and YOLO detection.

## ğŸ¯ Goals

Robots often fail in dim environments because visually â€œprettierâ€ images donâ€™t always help their perception stacks.  
BAMR focuses on **downstream metrics that matter**:

- **Geometric matching** â€” more keypoint correspondences & inliers (LoFTR).  
- **Detection** â€” higher mAP on ExDark under low light (YOLOv8).  

The model runs efficiently on embedded GPUs (T4/L4), supports AMP (fp16), and is Colab-friendly.

## What's in this repository
- **Stageâ€‘A**: paired enhancement (LOLâ€‘Blur / LOLâ€‘v2) with L1 + edge + perceptual.
- **Stageâ€‘B**: taskâ€‘aware tuning to **maximize matching** (LoFTR) and preserve edges/structure.
- **Endâ€‘toâ€‘end eval**: ExDark detector mAP and MID (Indoor/Outdoor) LoFTR matching, with Colabâ€‘ready scripts.

## Headline Results (What I achieved)

### LoFTR on MID-mini (10 pairs Ã— 3 frame-pairs)

| Condition | mean_matches | Î” vs orig | mean_inliers | Î” vs orig |
|:-----------|-------------:|-----------:|-------------:|-----------:|
| Original   | 1132.17 | â€” | 0.87 | â€” |
| Stage-A    | 1181.23 | **+4.3%** | 1.00 | **+15%** |
| **Stage-B** | **1277.93** | **+12.9%** | **1.57** | **+81%** |

*Outdoor inliers:* 0.07 â†’ **0.93 (~14Ã—)** with Stage-B.

### YOLOv8n on ExDark (validation)

| Condition | Description | mAP@50 | mAP@50-95 |
|:-----------|:-------------|:------:|:----------:|
| **Enhanced (before FT)** | detector evaluated on Stage-A outputs (no re-training) | 0.582 | 0.356 |
| **Enhanced (after FT)** | detector fine-tuned a few epochs on enhanced data | 0.626 | 0.376 |
| **Stage-B + YOLO adapt** | detector re-trained on Stage-B data | **0.644** | **0.408** |
| **Original (FT)** | fine-tuned on native ExDark | 0.675 | 0.416 |

> **FT = Fine-Tuning** â€” a short training run on the same (or enhanced) dataset to adapt the YOLO detector to new lighting distributions.

**Takeaway:** Stage-B consistently increases LoFTR robustness and nearly closes the detector gap with the original data after short adaptation.

## âš™ï¸ Quickstart

### Option A â€” Colab (recommended)

Open [`notebooks/00_quickstart_colab.py`](notebooks/00_quickstart_colab.py) and run cells top-to-bottom.

It will:
1. Mount Drive  
2. Prepare datasets (YOLO & paired lists)  
3. Train Stage-A / Stage-B  
4. Evaluate YOLO (original vs enhanced)  
5. Run LoFTR before/after comparisons.

### Option B â€” Local CLI

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# Prepare datasets
python scripts/10_prepare_exdark.py --project_root data/raw/exdark
python scripts/11_prepare_lol_blur.py
python scripts/12_prepare_lolv2.py
python scripts/13_prepare_mid_mini.py

# Stage-A (paired pretraining)
python scripts/20_train_stage_a.py --config configs/bamr_stage_a.yaml

# Stage-B (task-aware tuning)
python scripts/30_train_stage_b.py --config configs/bamr_stage_b.yaml

# Detector eval
python detector/yolo_val.py --data configs/exdark_yolo.yaml --name exdark_baseline
python detector/yolo_val.py --data configs/exdark_yolo_enh.yaml --name exdark_enh

# LoFTR eval on MID mini
python matching/loftr_eval_mid.py --project_root . --out results/tables/mid_loftr_before_after.csv
```
## Requirements
- Python â‰¥ 3.10
- CUDA-capable GPU (T4 or L4)
- See requirements.txt for libraries (Torch, Ultralytics, Kornia, OpenCV, NumPy, Pandas, PyYAML, TQDM)

## ğŸ“ Results Overview
- results_enh_val_before_ft.csv: YOLO val on enhanced (Stage-A) images before fine-tuning
- results_enh_val_after_ft.csv: YOLO val after short fine-tuning (FT) on enhanced images
- results_enh_val_after_taskaware.csv: YOLO val on Stage-B enhanced images (no FT or minimal)
- results_enh_val_after_taskaware_YOLO_adapt.csv: YOLO val after re-training detector on Stage-B data
- mid_loftr_before_after.csv: LoFTR matches & inliers (orig vs Stage-A vs Stage-B)

More details and tables are in results/README.md.

## ğŸ” Why This Matters
Traditional enhancers optimize pixel-fidelity metrics (PSNR, SSIM), which donâ€™t always improve perception. **BAMR** directly targets downstream robustness:
- Better geometric alignment between views â†’ stronger visual odometry.
- Improved low-light detection â†’ fewer missed obstacles or objects.
- Lightweight model suitable for embedded deployment.

## ğŸ§  Highlights & Lessons
- Stage-B loss mix (Î»_loftr=1.0, Î»_edge=0.6, Î»_rc=0.2) delivered the best geometric improvements.
- Mean inliers â†‘ 81% overall (MID mini).
- Detector adaptation on enhanced data recovers nearly all mAP lost to distribution shift.
- Simple AMP + stride padding allows stable training on Colab GPUs.

## ğŸ’¡ Future Work
- Add temporal consistency for video streams.
- Explore mixed fine-tuning (50 % original + 50 % Stage-B images) for detector robustness
- Integrate pose metrics (ATE/RPE) via VO/SLAM downstream tasks.
- Deploy TinyBAMR as an ONNX/TensorRT module on embedded hardware.

## Note: 
Found a bug or have an improvement? Contributions are welcome! ğŸ™Œ
1. **Open an issue** with a minimal repro (dataset slice, command line, logs).
2. **Submit a Merge Request / Pull Request** referencing the issue.
3. Please follow this checklist:
   - Clear description of the bug/fix
   - Steps to reproduce (commands, args, env details)
   - Affected files

I will review merge requests and leave feedback or merge when ready. Thanks for helping improve the project! ğŸš€

## Repository layout
```text
bamr-lowlight-robotics/
â”œâ”€ README.md
â”œâ”€ LICENSE
â”œâ”€ requirements.txt
â”œâ”€ .gitignore
â”œâ”€ bamr/                              # lightweight Python package
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ models.py                       # TinyBAMR model (Stageâ€‘A/B)
â”‚  â”œâ”€ losses.py                       # L1, edge/TV, perceptual, LoFTR/RC terms
â”‚  â”œâ”€ data.py                         # ExDark/LOLâ€‘Blur/LOLâ€‘v2/MID datasets + loaders
â”‚  â”œâ”€ train_stage_a.py                # paired pretraining (PSNRâ€‘oriented)
â”‚  â”œâ”€ train_stage_b.py                # taskâ€‘aware tuning (LoFTR/RC)
â”‚  â”œâ”€ eval_utils.py                   # PSNR/SSIM, small viz helpers
â”‚  â””â”€ utils.py                        # IO, seeding, logging, amp helpers
â”œâ”€ detector/
â”‚  â”œâ”€ yolo_utils.py                   # wrapper around Ultralytics API
â”‚  â”œâ”€ yolo_val.py                     # validate baseline/enhanced sets (JSON/CSV)
â”‚  â””â”€ yolo_finetune.py                # short FT on original/enhanced/mixed
â”œâ”€ matching/
â”‚  â”œâ”€ loftr_eval_mid.py               # memoryâ€‘safe â€œbefore vs afterâ€ MID evaluation
â”œâ”€ scripts/                           # CLI entrypoints you can run endâ€‘toâ€‘end
â”‚  â”œâ”€ 10_prepare_exdark.py
â”‚  â”œâ”€ 11_prepare_lol_blur.py
â”‚  â”œâ”€ 12_prepare_lolv2.py
â”‚  â”œâ”€ 13_prepare_mid_mini.py
â”‚  â”œâ”€ 20_train_stage_a.py
â”‚  â”œâ”€ 30_train_stage_b.py
â”‚  â”œâ”€ 40_eval_yolo_baseline_and_enh.py
â”‚  â”œâ”€ 41_yolo_finetune_mix.py
â”‚  â””â”€ 50_loftr_mid_before_after.py
â”œâ”€ configs/
â”‚  â”œâ”€ bamr_stage_a.yaml               # hyperparams for Stageâ€‘A
â”‚  â”œâ”€ bamr_stage_b.yaml               # hyperparams for Stageâ€‘B (loss weights, Î³)
â”‚  â”œâ”€ exdark_yolo.yaml                # original val/test paths
â”‚  â”œâ”€ exdark_yolo_enh.yaml            # enhanced val/test paths
â”‚  â””â”€ paths_example.yaml              # example local/Drive paths for data
â”œâ”€ notebooks/                         # Colabâ€‘first UX
â”‚  â”œâ”€ 00_quickstart_colab.py
â”‚  â”œâ”€ 01_prepare_datasets.py
â”‚  â”œâ”€ 02_train_stage_a.py
â”‚  â”œâ”€ 03_train_stage_b.py
â”‚  â”œâ”€ 04_yolo_eval_and_ft.py
â”‚  â””â”€ 05_loftr_mid_eval.py
â”œâ”€ results/
â”‚  â”œâ”€ README.md                       # comprehensive results
â”‚  â”œâ”€ tables/
â”‚  â”‚  â”œâ”€ results_enh_val_before_ft.csv
â”‚  â”‚  â”œâ”€ results_enh_val_after_ft.csv
â”‚  â”‚  â”œâ”€ results_enh_val_after_taskaware.csv
â”‚  â”‚  â”œâ”€ results_enh_val_after_taskaware_YOLO_adapt.csv
â”‚  â”‚  â””â”€ mid_loftr_before_after.csv
â”‚  â”œâ”€ figs/
â”‚  â”‚  â”œâ”€ EnhancedSamples.png
â””â”€ docs/
   â”œâ”€ datasets.md                     # links & expected folder layout for raw data
   â””â”€ design_notes.md                 # brief design choices and limitations
```
